{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image: np.ndarray, input_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the input image for YOLOv10 model.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image.\n",
    "        input_shape (tuple): The shape of the input tensor.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The preprocessed image.\n",
    "    \"\"\"\n",
    "    # Resize and normalize the image\n",
    "    image_resized = cv2.resize(image, (input_shape[2], input_shape[3]))\n",
    "    image_normalized = image_resized / 255.0\n",
    "    image_transposed = np.transpose(image_normalized, (2, 0, 1))\n",
    "    image_expanded = np.expand_dims(image_transposed, axis=0)\n",
    "    return image_expanded.astype(np.float32)\n",
    "\n",
    "\n",
    "def postprocess(preds: np.ndarray, max_det: int, nc: int = 80, conf_thres: float = 0.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Post-processes the predictions obtained from a YOLOv10 model.\n",
    "\n",
    "    Args:\n",
    "        preds (np.ndarray): The predictions obtained from the model. It should have a shape of (batch_size, num_boxes, 4 + num_classes).\n",
    "        max_det (int): The maximum number of detections to keep.\n",
    "        nc (int, optional): The number of classes. Defaults to 80.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): The post-processed predictions with shape (batch_size, max_det, 6),\n",
    "            including bounding boxes, scores, and cls.\n",
    "    \"\"\"\n",
    "    boxes, scores = np.split(preds, [4], axis=-1)\n",
    "    scores = scores[:, :, 1:]  # Ignore background score\n",
    "\n",
    "    max_scores = scores.max(axis=-1)\n",
    "    max_scores_indices = np.argsort(-max_scores, axis=-1)[:, :max_det]\n",
    "\n",
    "    boxes = np.array([boxes[i, max_scores_indices[i]] for i in range(boxes.shape[0])])\n",
    "    scores = np.array([scores[i, max_scores_indices[i]] for i in range(scores.shape[0])])\n",
    "    max_scores = np.array([max_scores[i, max_scores_indices[i]] for i in range(max_scores.shape[0])])\n",
    "\n",
    "    scores_flat = scores.reshape(scores.shape[0], -1)\n",
    "    topk_indices = np.argsort(-scores_flat, axis=-1)[:, :max_det]\n",
    "    topk_scores = np.array([scores_flat[i, topk_indices[i]] for i in range(scores_flat.shape[0])])\n",
    "    labels = topk_indices % nc\n",
    "    box_indices = topk_indices // nc\n",
    "    selected_boxes = np.array([boxes[i, box_indices[i]] for i in range(boxes.shape[0])])\n",
    "\n",
    "    # Filter out detections with low confidence\n",
    "    mask = (topk_scores > conf_thres).flatten()\n",
    "    selected_boxes = selected_boxes[:, mask, :]\n",
    "    topk_scores = topk_scores[:, mask]\n",
    "    labels = labels[:, mask]\n",
    "\n",
    "    # Transform bbox coordinates to x1y1x2y2 format\n",
    "    selected_boxes[:, :, :2] -= selected_boxes[:, :, 2:] / 2\n",
    "    selected_boxes[:, :, 2:] += selected_boxes[:, :, :2]\n",
    "\n",
    "    return np.concatenate(\n",
    "        [selected_boxes, topk_scores[..., None], labels[..., None].astype(selected_boxes.dtype)],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_yolo_outputs(outputs, strides, anchors):\n",
    "    output = None\n",
    "    for x, s, a in zip(outputs, strides, anchors):\n",
    "        out = parse_yolo_output(x, s, a)\n",
    "        output = out if output is None else np.concatenate((output, out), axis = 1)\n",
    "        \n",
    "    return output\n",
    "\n",
    "def parse_yolo_output(x, stride, anchors = None):\n",
    "    na = 1 if anchors is None else len(anchors)\n",
    "    bs, _, ny, nx = x.shape\n",
    "    grid = make_grid_numpy(nx, ny, na)\n",
    "    x = x.reshape(bs, na, -1, ny, nx).transpose((0, 1, 4, 3, 2))\n",
    "    #x[..., 0:2] = (x[..., 0:2] + grid) * stride  # xy\n",
    "    #x[..., 2:4] = np.exp(x[..., 2:4]) * stride # wh\n",
    "\n",
    "    x1y1 = grid - x[..., 0:2] + 0.5\n",
    "    x2y2 = grid + x[..., 2:4] + 0.5\n",
    "\n",
    "    c_xy = (x1y1 + x2y2) / 2\n",
    "    wh = x2y2 - x1y1\n",
    "    x[..., 0:2] = c_xy * stride\n",
    "    x[..., 2:4] = wh * stride\n",
    "    \n",
    "    x = x.reshape(bs, ny * nx, -1)\n",
    "    return x\n",
    "\n",
    "def make_grid_numpy(ny, nx, na):\n",
    "    y, x = np.arange(ny), np.arange(nx)\n",
    "    yv, xv = np.meshgrid(y, x, indexing='ij')\n",
    "    #grid = np.stack((xv, yv), 2).reshape(1, na, nx, ny, 2)\n",
    "    grid = np.stack((yv, xv), 2).reshape(1, na, ny, nx, 2)\n",
    "    return grid\n",
    "\n",
    "# Inference function\n",
    "def infer(image: np.ndarray, max_det: int = 10, nc: int = 80, session: ort.InferenceSession = None, input_name: str = None, output_names: list = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform inference on the input image using the YOLOv10 ONNX model.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image.\n",
    "        max_det (int, optional): The maximum number of detections to keep. Defaults to 100.\n",
    "        nc (int, optional): The number of classes. Defaults to 80.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The post-processed predictions.\n",
    "    \"\"\"\n",
    "    input_shape = session.get_inputs()[0].shape\n",
    "    print(\"Input shape:\", input_shape)\n",
    "    image_preprocessed = preprocess(image, input_shape)\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = session.run(output_names, {input_name: image_preprocessed})\n",
    "\n",
    "    output = parse_yolo_outputs(outputs, [8, 16, 32], [None, None, None])\n",
    "    # Combine outputs and convert to torch.Tensor\n",
    "    #combined_output = np.concatenate(outputs, axis=1)\n",
    "    #preds = torch.tensor(combined_output)\n",
    "\n",
    "    print(output.shape)\n",
    "    \n",
    "    # Post-process predictions\n",
    "    result = postprocess(output, max_det, nc)\n",
    "    print(result.shape)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def visualize(image: np.ndarray, results: torch.Tensor, class_names: list):\n",
    "    \"\"\"\n",
    "    Visualize the detection results on the image.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The original image.\n",
    "        results (torch.Tensor): The detection results.\n",
    "        class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    for det in results[0]:\n",
    "        print(det)\n",
    "        box = det[:4].astype(int)\n",
    "        score = det[4]\n",
    "        cls_id = int(det[5])\n",
    "        \n",
    "        # Draw bounding box\n",
    "        print(box)\n",
    "        print(score)\n",
    "        cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        #cv2.rectangle(image, (box[0] - box[2] // 2, box[1] - box[3] // 2), (box[0] + box[2] // 2, box[1] + box[3] // 2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw label and score\n",
    "        label = f\"{class_names[cls_id]}: {score:.2f}\"\n",
    "        cv2.putText(image, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images [1, 3, 640, 640]\n",
      "Input shape: [1, 3, 640, 640]\n",
      "(1, 8400, 85)\n",
      "(1, 2, 6)\n",
      "[179.37454    47.628754  407.9586    245.2543      0.8831582  14.       ]\n",
      "[179  47 407 245]\n",
      "0.8831582\n",
      "[  5.6466675 143.55789   640.15515   615.3313      0.8165297   1.       ]\n",
      "[  5 143 640 615]\n",
      "0.8165297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "model_path = 'tmp/yolov10n.onnx'\n",
    "session = ort.InferenceSession(model_path)\n",
    "\n",
    "print(session.get_inputs()[0].name, session.get_inputs()[0].shape)\n",
    "\n",
    "# Define the input and output names\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_names = [session.get_outputs()[i].name for i in range(3)]\n",
    "\n",
    "\n",
    "# Load an image\n",
    "image_path = 'tmp/image.jpg'\n",
    "class_names = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\", \n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \n",
    "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \n",
    "    \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform inference\n",
    "results = infer(image, session=session, input_name=input_name, output_names=output_names)\n",
    "\n",
    "# Visualize results\n",
    "image = cv2.resize(image, (640, 640))\n",
    "image_with_detections = visualize(image, results, class_names)\n",
    "\n",
    "# Display the image\n",
    "# cv2.imshow('Detections', image_with_detections)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# Save the image with detections\n",
    "output_image_path = 'tmp/image_with_detections.jpg'\n",
    "cv2.imwrite(output_image_path, image_with_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
